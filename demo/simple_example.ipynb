{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c16de3f-0258-406a-8213-9298805374e6",
   "metadata": {},
   "source": [
    "Before we start the simulation, we need to prepare the dataset. If there is not data in the given path, the dataset API will automatically download the correspnding data. In this example, we will use the Benchmark Dataset as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b10df751-f3bc-4379-8947-0c47fdb985c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SSVEPAnalysisToolbox.datasets import BenchmarkDataset\n",
    "dataset = BenchmarkDataset(path = '2016_Tsinghua_SSVEP_database')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d96b625-d230-4f12-bada-4051b9931363",
   "metadata": {},
   "source": [
    "Because EEG signals normally contain large noise, we need do preprocesses when we extract signals. Therefore, we need hook the preprocess method on the dataset. The Benchmark Dataset paper already provides the suggested preprocess methods. These method has been included in this toolbox and can be directly used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd0a8fe-946d-4324-bed8-41ed2b4d06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SSVEPAnalysisToolbox.utils.benchmarkpreprocess import preprocess\n",
    "dataset.regist_preprocess(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e610373-8add-45d8-8121-9983d351a22d",
   "metadata": {},
   "source": [
    "Because the filter-bank approach has been successfully adopted to improve the recognition performance in literature, we need to hook the filter-bank method on the dataset. The Benchmark Dataset paper already provides the suggested filter-bank method. This method has also been included in this toolbox and can be directly used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "108b4665-44f4-49fe-81c3-3f2c5b943988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SSVEPAnalysisToolbox.utils.benchmarkpreprocess import filterbank\n",
    "dataset.regist_filterbank(filterbank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0aff9-a895-4524-ab88-39fcf4210d09",
   "metadata": {},
   "source": [
    "After preparing the dataset, we need to prepare the recognition method. The toolbox contains various methods with different implementations. This example use the eCCA method as an example to show how to use the method API. In addition, because we use the filter-bank approach, we need to predefine the weights of different filter banks. The Benchmark Dataset paper already provides the suggested weights. The method of generating these weights has been implemented in this toolbox and can be directly used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac89d6f8-aff5-4259-af88-fcfc1af652fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SSVEPAnalysisToolbox.utils.benchmarkpreprocess import suggested_weights_filterbank\n",
    "weights_filterbank = suggested_weights_filterbank()\n",
    "from SSVEPAnalysisToolbox.algorithms import ETRCA\n",
    "recog_model = ETRCA(weights_filterbank = weights_filterbank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee53c5b-511e-4b50-9eaa-34e6bb5b7be7",
   "metadata": {},
   "source": [
    "Now, we can prepare the simulation. In this example, \n",
    "\n",
    "1. we will only use 9 occipital channels;\n",
    "2. All 40 classes in the Benchmark data are considered.\n",
    "3. 5 harmonic components are considered in the SSVEP reference signals;\n",
    "4. The first 1 second EEG signals after removing 0.14s latency are applied for this example;\n",
    "5. Only the second subject's EEG is used for the individual recognition;\n",
    "6. EEG signals in the first block is used for testing the recognition method;\n",
    "7. EEG signals in other blocks is used for training the recognition method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c6900ed-43b9-40c5-818e-ae5ed24ff591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SSVEPAnalysisToolbox.utils.benchmarkpreprocess import suggested_ch\n",
    "ch_used = suggested_ch()\n",
    "all_trials = [i for i in range(dataset.trial_num)]\n",
    "harmonic_num = 5\n",
    "tw = 1\n",
    "sub_idx = 2-1\n",
    "test_block_idx = 0\n",
    "test_block_list, train_block_list = dataset.leave_one_block_out(block_idx = test_block_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b066300d-2bf3-4e5c-a7f0-447814f5e2a7",
   "metadata": {},
   "source": [
    "The whole simulation is divided into 2 steps:\n",
    "\n",
    "1. Train the recognition model:\n",
    "   \n",
    "   1. Prepare the training materials: The training process of most recognition methods requires the training data, corresponding labels, the SSVEP reference signals (sine-cosine reference signals), and freqeucies of labels. Although the eCCA does not need freqeucies of labels, we still show how to prepare and input them.\n",
    "   2. Use the training materials to train the model. We also show how to record the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "543e1e27-051f-4b9f-876f-6a8323950569",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sig = dataset.get_ref_sig(tw, harmonic_num)\n",
    "freqs = dataset.stim_info['freqs']\n",
    "X_train, Y_train = dataset.get_data(sub_idx = sub_idx,\n",
    "                                    blocks = train_block_list,\n",
    "                                    trials = all_trials,\n",
    "                                    channels = ch_used,\n",
    "                                    sig_len = tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89993285-2bc9-4a7d-b5e0-82de578019cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tic = time.time()\n",
    "recog_model.fit(X=X_train, Y=Y_train, ref_sig=ref_sig, freqs=freqs) \n",
    "toc_train = time.time()-tic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068cc96f-5be7-45e5-b6ff-c1eb10b02da6",
   "metadata": {},
   "source": [
    "2. Test the recognition model:\n",
    "\n",
    "   1. Prepare the testing materials: Normally, we only need the testing EEG signals. But we also extract the corresponding testing labels for further calculating classification accuracy;\n",
    "   2. Use the testing materials to test the model. We also record the testing time and compute the averaged testing time of each trial for further calculating the ITR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef63b92d-150d-4b43-b07a-b0566f73401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = dataset.get_data(sub_idx = sub_idx,\n",
    "                                    blocks = test_block_list,\n",
    "                                    trials = all_trials,\n",
    "                                    channels = ch_used,\n",
    "                                    sig_len = tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42f2db6e-03fc-4c09-9d72-436fd32f05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "pred_label, _ = recog_model.predict(X_test)\n",
    "toc_test = time.time()-tic\n",
    "toc_test_onetrial = toc_test/len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc2a7b-eadd-4ced-8a26-3284b424d56d",
   "metadata": {},
   "source": [
    "Finally, we can use the build-in functions to quickly calculate the classification accuracy and ITR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68f130d5-856e-4e55-b522-649122397803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulation Information:\n",
      "    Method Name: eTRCA\n",
      "    Dataset: Benchmark Dataset\n",
      "    Signal length: 1.000 s\n",
      "    Channel: [47, 53, 54, 55, 56, 57, 60, 61, 62]\n",
      "    Subject index: 1\n",
      "    Testing block: [0]\n",
      "    Training block: [1, 2, 3, 4, 5]\n",
      "    Training time: 0.07302 s\n",
      "    Total Testing time: 1.12959 s\n",
      "    Testing time of single trial: 0.02824 s\n",
      "\n",
      "Performance:\n",
      "    Acc: 97.500 %\n",
      "    ITR: 180.590 bits/min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from SSVEPAnalysisToolbox.evaluator import cal_acc,cal_itr\n",
    "acc = cal_acc(Y_true = Y_test, Y_pred = pred_label)\n",
    "itr = cal_itr(tw = tw, t_break = dataset.t_break, t_latency = dataset.default_t_latency, t_comp = toc_test_onetrial,\n",
    "              N = len(freqs), acc = acc)\n",
    "print(\"\"\"\n",
    "Simulation Information:\n",
    "    Method Name: {:s}\n",
    "    Dataset: {:s}\n",
    "    Signal length: {:.3f} s\n",
    "    Channel: {:s}\n",
    "    Subject index: {:n}\n",
    "    Testing block: {:s}\n",
    "    Training block: {:s}\n",
    "    Training time: {:.5f} s\n",
    "    Total Testing time: {:.5f} s\n",
    "    Testing time of single trial: {:.5f} s\n",
    "\n",
    "Performance:\n",
    "    Acc: {:.3f} %\n",
    "    ITR: {:.3f} bits/min\n",
    "\"\"\".format(recog_model.ID,\n",
    "           dataset.ID,\n",
    "           tw,\n",
    "           str(ch_used),\n",
    "           sub_idx,\n",
    "           str(test_block_list),\n",
    "           str(train_block_list),\n",
    "           toc_train,\n",
    "           toc_test,\n",
    "           toc_test_onetrial,\n",
    "           acc*100,\n",
    "           itr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
